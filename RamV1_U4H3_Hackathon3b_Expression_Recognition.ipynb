{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aouca44eWv1DNkYe7bNp85BE_zxXdyxu","timestamp":1731738769877}]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SZIubkln0AI2"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"a8pCC9bE2blm"},"source":["Automated facial expression recognition provides an objective assessment of emotions. Human based assessment of emotions has many limitations and biases and automated facial expression technology has been found to deliver a better level of insight into behavior patterns. Emotion detection from facial expressions using AI is useful in automatically measuring consumers’ engagement with their content and brands, audience engagement for advertisements, customer satisfaction in the retail sector, psychological analyses, law enforcement etc."]},{"cell_type":"code","metadata":{"id":"RURW82wHOKnb"},"source":["#@title Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Hackathon3b_expression_recognition.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CrBFZZit7ES0"},"source":["**Objectives:**\n","\n","**Stage 4 (15 Marks):** Train a CNN Model and perform Expression Recognition in the EFR Mobile App.\n","\n","**Stage 5 (5 Marks):** Test for Anti-Face Spoofing on the EFR Mobile App."]},{"cell_type":"markdown","metadata":{"id":"BGq6XpvhFynP"},"source":["##**Stage 4 (15 Marks)**\n","\n","**(i) Train a CNN Model for Expression Recognition on given Expression data  \n","(ii) Deploy the Model and Perform Expression Recognition on Team Data through the EFR Mobile App**\n","\n","\n","---\n","\n","\n","* Define and train a CNN for expression recognition for the data under folder \"Expression_data\" which segregated on expression basis.\n","* Collect your team data using EFR application and test your model on the same and optimize the CNN architecture for predicting the respective labels of the images.\n","* Save and Download the trained expression model and upload them in the ftp server (refer to [Filezilla Installation and Configuration document](https://drive.google.com/file/d/19UIKpyVK4r12Dxklo8quQdZQ31PWpiKM/view?usp=drive_link)).\n","\n","* Update the **“exp_recognition.py”** file in the server. Open the files in the terminal (Command prompt) and provide the code for predicting the expression on the face (Note: To define the architecture of your trained model, you'll need to define it in the file **\"exp_recognition_model.py\"**).\n","\n","* Test your model on the mobile app for Expression Recognition and Sequence Expression. Your team can also see your results in your terminal.\n","\n","\n","* Grading Scheme:\n","> * Expression Recognition (12M): If the functionality is returning expression class correctly for the face using the mobile app’s “Expression Recognition” functionality\n","> * Sequence Expression (3M): Get three consecutive correct Expressions using the mobile app’s “Sequence Expressions” functionality"]},{"cell_type":"markdown","metadata":{"id":"3e0e3sFh0JZJ"},"source":["**Download the dataset**"]},{"cell_type":"code","metadata":{"id":"Kv0xxq_d0Qb_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731764807754,"user_tz":-330,"elapsed":219022,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"5811f803-c2ea-4f65-df50-c6a7c21afbb4"},"source":["#@title Run this cell to download the dataset\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","\n","notebook=\"M3_Hackathon\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx wget wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Expression_data.zip\")\n","\n","    ipython.magic(\"sx unzip Expression_data.zip\")\n","\n","    ipython.magic(\"sx pip install torch==2.5.1 -f https://download.pytorch.org/whl/cu100/stable\")\n","    ipython.magic(\"sx pip install torchvision==0.11\")\n","    ipython.magic(\"sx pip install opencv-python\")\n","    print (\"Setup completed successfully\")\n","    return\n","setup()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup completed successfully\n"]}]},{"cell_type":"markdown","metadata":{"id":"NhLFY4n6BwIj"},"source":["**Dataset attributes:**\n","\n","During the setup you have downloaded the Expression data:\n","\n","* **Expression_data**: In this folder, the images are segregrated in terms of Expression\n","> * Expressions available: ANGER, DISGUST, FEAR, HAPPINESS, NEUTRAL, SADNESS, SURPRISE\n","> * Each class is organised as one folder\n","> * There are ~18000 total images in the training data and ~4500 total images in the testing data"]},{"cell_type":"code","metadata":{"id":"9llRIwuyNxMZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731764831530,"user_tz":-330,"elapsed":619,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"2d081f42-866d-4f1c-f3c5-8c9fcba228d1"},"source":["%ls"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mExpression_data\u001b[0m/  Expression_data.zip  \u001b[01;34m__MACOSX\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"markdown","metadata":{"id":"RC9IMSrJGTjK"},"source":["**Imports: All the imports are defined here**\n","\n"]},{"cell_type":"code","metadata":{"id":"Lj0Yjxe8G46e","executionInfo":{"status":"ok","timestamp":1731764838812,"user_tz":-330,"elapsed":5011,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"source":["%matplotlib inline\n","import torchvision\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader,Dataset\n","import matplotlib.pyplot as plt\n","import torchvision.utils\n","import numpy as np\n","import random\n","from PIL import Image\n","import torch\n","from torch.autograd import Variable\n","import PIL.ImageOps\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import os\n","import warnings\n","from time import sleep\n","import sys\n","warnings.filterwarnings('ignore')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWe3lAlQk5U0"},"source":["For the following step, to obtain hints on building a CNN model for face expression, you may refer to this [article](https://drive.google.com/open?id=1P2rpaWW3tOtGGnw4dvtdZ4hjoc8iDNst)"]},{"cell_type":"markdown","metadata":{"id":"c86_KxLjmLT7"},"source":["**Define and train a CNN model for expression recognition**"]},{"cell_type":"code","source":[],"metadata":{"id":"jua9J4qvrQLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class ExpressionCNN(nn.Module):\n","#     def __init__(self):\n","#         super(ExpressionCNN, self).__init__()\n","#         # Convolutional Layers\n","#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n","#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","#         self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n","\n","#         # Batch Normalization\n","#         self.bn1 = nn.BatchNorm2d(32)\n","#         self.bn2 = nn.BatchNorm2d(64)\n","#         self.bn3 = nn.BatchNorm2d(128)\n","#         self.bn4 = nn.BatchNorm2d(256)\n","\n","#         # Dropout\n","#         self.dropout = nn.Dropout(0.5)\n","\n","#         # Fully Connected Layers\n","#         self.fc1 = nn.Linear(256 * 3 * 3, 512)  # Modified FC1\n","#         self.fc2 = nn.Linear(512, 256)\n","#         self.fc3 = nn.Linear(256, 7)  # 7 classes for expressions\n","\n","#     def forward(self, x):\n","#         # Convolutional layers with ReLU, BatchNorm, and MaxPooling\n","#         x = F.relu(self.bn1(self.conv1(x)))\n","#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","#         x = F.relu(self.bn2(self.conv2(x)))\n","#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","#         x = F.relu(self.bn3(self.conv3(x)))\n","#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","#         x = F.relu(self.bn4(self.conv4(x)))\n","#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","#         # Flatten\n","#         x = x.view(x.size(0), -1)\n","\n","#         # Fully connected layers with ReLU and Dropout\n","#         x = F.relu(self.fc1(x))\n","#         x = self.dropout(x)\n","\n","#         x = F.relu(self.fc2(x))\n","#         x = self.dropout(x)\n","\n","#         # Output layer for classification\n","#         x = self.fc3(x)\n","#         return x\n","\n","# # Instantiate and print the model\n","# model = ExpressionCNN()\n","# print(model)"],"metadata":{"id":"udSZ20_c4mUT","executionInfo":{"status":"ok","timestamp":1731764842704,"user_tz":-330,"elapsed":438,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class ExpressionCNN(nn.Module):\n","    def __init__(self):\n","        super(ExpressionCNN, self).__init__()\n","        # Convolutional Layers\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=128, out_channels=364, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=364, out_channels=512, kernel_size=3, stride=1, padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=512, out_channels=720, kernel_size=3, stride=1, padding=1)\n","\n","\n","        # Batch Normalization\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.bn2 = nn.BatchNorm2d(128)\n","        self.bn3 = nn.BatchNorm2d(364)\n","        self.bn4 = nn.BatchNorm2d(512)\n","        self.bn5 = nn.BatchNorm2d(720)\n","\n","        # Dropout\n","        self.dropout = nn.Dropout(0.5)\n","\n","        # Fully Connected Layers\n","        self.fc1 = nn.Linear(720 * 1 * 1, 128)  # Modified FC1\n","        #self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(128, 7)  # 7 classes for expressions\n","\n","    def forward(self, x):\n","        # Convolutional layers with ReLU, BatchNorm, and MaxPooling\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","        x = F.relu(self.bn4(self.conv4(x)))\n","        x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","        x = F.relu(self.bn5(self.conv5(x)))\n","        x = F.max_pool2d(x, kernel_size=2, stride=2)\n","\n","        # Flatten\n","        x = x.view(x.size(0), -1)\n","\n","        # Fully connected layers with ReLU and Dropout\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","\n","        #x = F.relu(self.fc2(x))\n","        #x = self.dropout(x)\n","\n","        # Output layer for classification\n","        x = self.fc3(x)\n","        return x\n","\n","# Instantiate and print the model\n","model = ExpressionCNN()\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XA7n6xkRKHOp","executionInfo":{"status":"ok","timestamp":1731764969509,"user_tz":-330,"elapsed":722,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"1b2135d7-9d09-4244-a4a6-f9d3b39a9d97"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ExpressionCNN(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(128, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(364, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv5): Conv2d(512, 720, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn3): BatchNorm2d(364, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (bn5): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc1): Linear(in_features=720, out_features=128, bias=True)\n","  (fc3): Linear(in_features=128, out_features=7, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"gNZCxw0wmRlI"},"source":["**Test your model and optimize CNN architecture for predicting the labels correctly**"]},{"cell_type":"code","source":["#   to train CNN model for Expression_Data\n","\n","\n","# Define the data transformations\n","transform = transforms.Compose([\n","    #transforms.Grayscale(num_output_channels=1),\n","    transforms.Resize((48, 48)),\n","    transforms.ToTensor(),\n","    #transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Load the dataset\n","train_dataset = torchvision.datasets.ImageFolder(root='/content/Expression_data/Facial_expression_train', transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device) # Assuming your model is already defined as 'model'\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","total, correct = 0, 0\n","num_epochs = 35  # Adjust as needed\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        if (i + 1) % 100 == 0:\n","            accuracy = 100 * correct / total\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')\n","            total, correct = 0, 0  # Reset for next batch\n","    if accuracy >= 90:\n","        break\n","\n","print(\"Training finished.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1HYVIzjwnRr","executionInfo":{"status":"ok","timestamp":1731765400628,"user_tz":-330,"elapsed":425047,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"481c21e9-421a-41d2-9a38-e2af00351f9f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/35], Step [100/569], Loss: 1.8158, Accuracy: 26.88%\n","Epoch [1/35], Step [200/569], Loss: 1.7670, Accuracy: 28.06%\n","Epoch [1/35], Step [300/569], Loss: 1.7312, Accuracy: 30.31%\n","Epoch [1/35], Step [400/569], Loss: 1.6919, Accuracy: 28.44%\n","Epoch [1/35], Step [500/569], Loss: 1.5854, Accuracy: 30.59%\n","Epoch [2/35], Step [100/569], Loss: 1.7934, Accuracy: 28.71%\n","Epoch [2/35], Step [200/569], Loss: 1.9218, Accuracy: 28.44%\n","Epoch [2/35], Step [300/569], Loss: 1.6438, Accuracy: 30.00%\n","Epoch [2/35], Step [400/569], Loss: 1.8126, Accuracy: 30.16%\n","Epoch [2/35], Step [500/569], Loss: 1.7983, Accuracy: 31.50%\n","Epoch [3/35], Step [100/569], Loss: 1.9035, Accuracy: 30.75%\n","Epoch [3/35], Step [200/569], Loss: 1.7922, Accuracy: 31.38%\n","Epoch [3/35], Step [300/569], Loss: 1.9528, Accuracy: 31.44%\n","Epoch [3/35], Step [400/569], Loss: 1.6949, Accuracy: 32.69%\n","Epoch [3/35], Step [500/569], Loss: 1.6136, Accuracy: 34.66%\n","Epoch [4/35], Step [100/569], Loss: 1.8031, Accuracy: 33.40%\n","Epoch [4/35], Step [200/569], Loss: 1.8217, Accuracy: 34.81%\n","Epoch [4/35], Step [300/569], Loss: 1.6703, Accuracy: 34.84%\n","Epoch [4/35], Step [400/569], Loss: 1.5521, Accuracy: 35.59%\n","Epoch [4/35], Step [500/569], Loss: 1.6581, Accuracy: 35.78%\n","Epoch [5/35], Step [100/569], Loss: 1.7622, Accuracy: 37.43%\n","Epoch [5/35], Step [200/569], Loss: 1.8223, Accuracy: 37.62%\n","Epoch [5/35], Step [300/569], Loss: 1.6165, Accuracy: 37.50%\n","Epoch [5/35], Step [400/569], Loss: 1.6839, Accuracy: 38.97%\n","Epoch [5/35], Step [500/569], Loss: 1.6511, Accuracy: 37.03%\n","Epoch [6/35], Step [100/569], Loss: 1.7332, Accuracy: 38.06%\n","Epoch [6/35], Step [200/569], Loss: 1.6548, Accuracy: 39.75%\n","Epoch [6/35], Step [300/569], Loss: 1.8806, Accuracy: 40.09%\n","Epoch [6/35], Step [400/569], Loss: 1.5253, Accuracy: 37.88%\n","Epoch [6/35], Step [500/569], Loss: 1.4801, Accuracy: 37.66%\n","Epoch [7/35], Step [100/569], Loss: 1.4734, Accuracy: 39.70%\n","Epoch [7/35], Step [200/569], Loss: 1.4093, Accuracy: 39.91%\n","Epoch [7/35], Step [300/569], Loss: 1.5208, Accuracy: 41.28%\n","Epoch [7/35], Step [400/569], Loss: 1.4126, Accuracy: 39.62%\n","Epoch [7/35], Step [500/569], Loss: 1.5516, Accuracy: 39.50%\n","Epoch [8/35], Step [100/569], Loss: 1.4533, Accuracy: 40.42%\n","Epoch [8/35], Step [200/569], Loss: 1.4676, Accuracy: 40.97%\n","Epoch [8/35], Step [300/569], Loss: 1.6953, Accuracy: 41.25%\n","Epoch [8/35], Step [400/569], Loss: 1.3305, Accuracy: 41.22%\n","Epoch [8/35], Step [500/569], Loss: 1.9345, Accuracy: 40.81%\n","Epoch [9/35], Step [100/569], Loss: 1.2752, Accuracy: 41.19%\n","Epoch [9/35], Step [200/569], Loss: 1.7103, Accuracy: 42.34%\n","Epoch [9/35], Step [300/569], Loss: 1.5439, Accuracy: 41.84%\n","Epoch [9/35], Step [400/569], Loss: 1.7286, Accuracy: 43.25%\n","Epoch [9/35], Step [500/569], Loss: 1.5245, Accuracy: 44.22%\n","Epoch [10/35], Step [100/569], Loss: 1.6869, Accuracy: 42.60%\n","Epoch [10/35], Step [200/569], Loss: 1.3852, Accuracy: 43.53%\n","Epoch [10/35], Step [300/569], Loss: 1.5776, Accuracy: 44.78%\n","Epoch [10/35], Step [400/569], Loss: 1.7829, Accuracy: 42.75%\n","Epoch [10/35], Step [500/569], Loss: 1.2258, Accuracy: 44.25%\n","Epoch [11/35], Step [100/569], Loss: 1.3829, Accuracy: 44.27%\n","Epoch [11/35], Step [200/569], Loss: 1.4397, Accuracy: 46.22%\n","Epoch [11/35], Step [300/569], Loss: 1.2183, Accuracy: 48.41%\n","Epoch [11/35], Step [400/569], Loss: 1.2370, Accuracy: 47.00%\n","Epoch [11/35], Step [500/569], Loss: 1.2886, Accuracy: 45.38%\n","Epoch [12/35], Step [100/569], Loss: 1.1075, Accuracy: 47.08%\n","Epoch [12/35], Step [200/569], Loss: 1.2138, Accuracy: 47.94%\n","Epoch [12/35], Step [300/569], Loss: 1.2392, Accuracy: 50.03%\n","Epoch [12/35], Step [400/569], Loss: 1.1881, Accuracy: 49.72%\n","Epoch [12/35], Step [500/569], Loss: 1.3940, Accuracy: 49.41%\n","Epoch [13/35], Step [100/569], Loss: 0.8760, Accuracy: 51.90%\n","Epoch [13/35], Step [200/569], Loss: 1.2371, Accuracy: 54.47%\n","Epoch [13/35], Step [300/569], Loss: 1.1857, Accuracy: 54.12%\n","Epoch [13/35], Step [400/569], Loss: 1.1063, Accuracy: 54.28%\n","Epoch [13/35], Step [500/569], Loss: 1.5135, Accuracy: 53.28%\n","Epoch [14/35], Step [100/569], Loss: 0.9346, Accuracy: 56.49%\n","Epoch [14/35], Step [200/569], Loss: 1.0052, Accuracy: 59.91%\n","Epoch [14/35], Step [300/569], Loss: 1.2013, Accuracy: 57.47%\n","Epoch [14/35], Step [400/569], Loss: 1.3494, Accuracy: 58.94%\n","Epoch [14/35], Step [500/569], Loss: 1.0816, Accuracy: 57.50%\n","Epoch [15/35], Step [100/569], Loss: 0.5805, Accuracy: 62.16%\n","Epoch [15/35], Step [200/569], Loss: 1.0195, Accuracy: 64.94%\n","Epoch [15/35], Step [300/569], Loss: 0.9300, Accuracy: 65.09%\n","Epoch [15/35], Step [400/569], Loss: 0.9078, Accuracy: 63.22%\n","Epoch [15/35], Step [500/569], Loss: 1.1836, Accuracy: 63.00%\n","Epoch [16/35], Step [100/569], Loss: 0.5583, Accuracy: 67.65%\n","Epoch [16/35], Step [200/569], Loss: 0.8771, Accuracy: 72.03%\n","Epoch [16/35], Step [300/569], Loss: 0.7622, Accuracy: 68.06%\n","Epoch [16/35], Step [400/569], Loss: 0.6221, Accuracy: 69.72%\n","Epoch [16/35], Step [500/569], Loss: 0.9006, Accuracy: 69.84%\n","Epoch [17/35], Step [100/569], Loss: 0.9455, Accuracy: 74.08%\n","Epoch [17/35], Step [200/569], Loss: 0.4979, Accuracy: 77.00%\n","Epoch [17/35], Step [300/569], Loss: 0.8985, Accuracy: 77.31%\n","Epoch [17/35], Step [400/569], Loss: 0.6398, Accuracy: 75.56%\n","Epoch [17/35], Step [500/569], Loss: 0.6984, Accuracy: 76.84%\n","Epoch [18/35], Step [100/569], Loss: 0.6119, Accuracy: 79.12%\n","Epoch [18/35], Step [200/569], Loss: 0.3842, Accuracy: 82.19%\n","Epoch [18/35], Step [300/569], Loss: 0.4180, Accuracy: 80.75%\n","Epoch [18/35], Step [400/569], Loss: 0.8226, Accuracy: 80.09%\n","Epoch [18/35], Step [500/569], Loss: 0.5920, Accuracy: 81.34%\n","Epoch [19/35], Step [100/569], Loss: 0.3302, Accuracy: 83.92%\n","Epoch [19/35], Step [200/569], Loss: 0.3176, Accuracy: 86.62%\n","Epoch [19/35], Step [300/569], Loss: 0.3932, Accuracy: 86.38%\n","Epoch [19/35], Step [400/569], Loss: 0.4395, Accuracy: 86.12%\n","Epoch [19/35], Step [500/569], Loss: 0.5102, Accuracy: 84.56%\n","Epoch [20/35], Step [100/569], Loss: 0.1478, Accuracy: 88.34%\n","Epoch [20/35], Step [200/569], Loss: 0.3837, Accuracy: 90.16%\n","Epoch [20/35], Step [300/569], Loss: 0.4467, Accuracy: 88.78%\n","Epoch [20/35], Step [400/569], Loss: 0.2643, Accuracy: 88.06%\n","Epoch [20/35], Step [500/569], Loss: 0.3136, Accuracy: 87.91%\n","Epoch [21/35], Step [100/569], Loss: 0.4550, Accuracy: 89.85%\n","Epoch [21/35], Step [200/569], Loss: 0.2068, Accuracy: 92.19%\n","Epoch [21/35], Step [300/569], Loss: 0.2792, Accuracy: 90.81%\n","Epoch [21/35], Step [400/569], Loss: 0.2650, Accuracy: 90.69%\n","Epoch [21/35], Step [500/569], Loss: 0.3628, Accuracy: 89.34%\n","Epoch [22/35], Step [100/569], Loss: 0.1601, Accuracy: 90.33%\n","Epoch [22/35], Step [200/569], Loss: 0.1755, Accuracy: 92.38%\n","Epoch [22/35], Step [300/569], Loss: 0.1252, Accuracy: 92.19%\n","Epoch [22/35], Step [400/569], Loss: 0.3442, Accuracy: 92.53%\n","Epoch [22/35], Step [500/569], Loss: 0.2654, Accuracy: 92.31%\n","Training finished.\n"]}]},{"cell_type":"code","source":["# Save the model\n","torch.save(model.state_dict(), 'expression_model.pth')"],"metadata":{"id":"qLbrkcD-0xf1","executionInfo":{"status":"ok","timestamp":1731765415667,"user_tz":-330,"elapsed":504,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["test_dataset = torchvision.datasets.ImageFolder(root='/content/Expression_data/Facial_expression_test', transform=transform)\n","test_loader = DataLoader(test_dataset , batch_size=32, shuffle=True)"],"metadata":{"id":"icZFL1HzyxGT","executionInfo":{"status":"ok","timestamp":1731765420261,"user_tz":-330,"elapsed":615,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Load the saved model\n","model = ExpressionCNN()  # Assuming ExpressionCNN is defined in your code\n","model.load_state_dict(torch.load('expression_model.pth'))\n","model.eval()  # Set the model to evaluation mode\n","\n","# Define the device (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Testing loop\n","correct = 0\n","total = 0\n","with torch.no_grad():  # Disable gradient calculations during testing\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","test_accuracy = 100 * correct / total\n","print(f\"Test Accuracy: {test_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I80W6zO9Fzad","executionInfo":{"status":"ok","timestamp":1731765429057,"user_tz":-330,"elapsed":6909,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"a4e904ad-aa05-4e76-ad7d-1c0b168730c3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 42.15%\n"]}]},{"cell_type":"code","metadata":{"id":"SzuYlK_8mbYy"},"source":["# YOUR CODE HERE for test evaluation\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvw3vZ7kR9sR"},"source":["**Team Data Collection (activate the server first)**\n","\n","  - (This can be done on the day of the Hackathon once the login username and password are given)"]},{"cell_type":"markdown","metadata":{"id":"1ki9ee8YUFQr"},"source":["Activate the Server Access\n","* Open the terminal (Command Prompt)\n","* Login to SSH by typing **ssh (username)@aiml-sandbox1.talentsprint.com**. Give the login username which is given to you.\n","\n","Eg: `ssh b16h3gxx@aiml-sandbox1.talentsprint.com`\n","\n","  (If it is your first time connecting to the server from this computer, accept the connection by typing \"yes\".)\n","* After logging into SSH, please activate your virtual environment using the\n","command **source venv/bin/activate** and then press enter\n","* You can start the server by giving the command **sh runserver.sh** and then press enter.\n","* In order to collect team data in mobile app, ensure the server is active\n"]},{"cell_type":"markdown","metadata":{"id":"bHpqJDuFHVmL"},"source":["**Collect your team data using the EFR Mobile App and fine-tune the CNN for expression data on your team**\n","\n","Team Data Collection\n","\n","* Follow the \"Mobile_APP_Documentation\" to collect the Expression photos of your team. These will be stored in the server to which login is provided to you.\n","\n","[Mobile_APP_Documentation](https://drive.google.com/file/d/1F9SU-BwKViK_eZV2-P3pymvGUILUoVFf/view?usp=drive_link)\n","\n","\n","**Download your team expression data from the EFR app into your colab notebook using the links provided below**\n","\n","NOTE: Replace the string \"username\" with your login username (such as b16h3gxx) in the below cell for expression images.\n","\n","This data will be useful for testing the above trained cnn networks."]},{"cell_type":"code","metadata":{"id":"I8U0F_CDIhmh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731765457575,"user_tz":-330,"elapsed":13672,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"5090ab4a-1bd2-4462-b282-8f93574cf733"},"source":["!wget -nH --recursive --no-parent --reject 'index.*' https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ --cut-dirs=3  -P ./captured_images_with_Expression"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-11-16 13:57:23--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/\n","Resolving aiml-sandbox.talentsprint.com (aiml-sandbox.talentsprint.com)... 139.162.203.12\n","Connecting to aiml-sandbox.talentsprint.com (aiml-sandbox.talentsprint.com)|139.162.203.12|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/index.html.tmp’\n","\n","index.html.tmp          [ <=>                ]   1.01K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:24 (225 MB/s) - ‘./captured_images_with_Expression/index.html.tmp’ saved [1038]\n","\n","Loading robots.txt; please ignore errors.\n","--2024-11-16 13:57:24--  https://aiml-sandbox.talentsprint.com/robots.txt\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 404 Not Found\n","2024-11-16 13:57:24 ERROR 404: Not Found.\n","\n","Removing ./captured_images_with_Expression/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:24--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/ANGER/index.html.tmp’\n","\n","ANGER/index.html.tm     [ <=>                ]   1.96K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:24 (464 MB/s) - ‘./captured_images_with_Expression/ANGER/index.html.tmp’ saved [2011]\n","\n","Removing ./captured_images_with_Expression/ANGER/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:24--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/index.html.tmp’\n","\n","DISGUST/index.html.     [ <=>                ]   1.62K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:24 (522 MB/s) - ‘./captured_images_with_Expression/DISGUST/index.html.tmp’ saved [1662]\n","\n","Removing ./captured_images_with_Expression/DISGUST/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:24--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/FEAR/index.html.tmp’\n","\n","FEAR/index.html.tmp     [ <=>                ]   1.34K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:24 (306 MB/s) - ‘./captured_images_with_Expression/FEAR/index.html.tmp’ saved [1375]\n","\n","Removing ./captured_images_with_Expression/FEAR/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:24--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/index.html.tmp’\n","\n","HAPPINESS/index.htm     [ <=>                ]   2.28K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:25 (84.3 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/index.html.tmp’ saved [2333]\n","\n","Removing ./captured_images_with_Expression/HAPPINESS/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:25--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/index.html.tmp’\n","\n","NEUTRAL/index.html.     [ <=>                ]   1.62K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:25 (551 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/index.html.tmp’ saved [1662]\n","\n","Removing ./captured_images_with_Expression/NEUTRAL/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:25--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/index.html.tmp’\n","\n","SADNESS/index.html.     [ <=>                ]   1.75K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:25 (160 MB/s) - ‘./captured_images_with_Expression/SADNESS/index.html.tmp’ saved [1789]\n","\n","Removing ./captured_images_with_Expression/SADNESS/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:25--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/index.html.tmp’\n","\n","SURPRISE/index.html     [ <=>                ]   1.89K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:25 (452 MB/s) - ‘./captured_images_with_Expression/SURPRISE/index.html.tmp’ saved [1931]\n","\n","Removing ./captured_images_with_Expression/SURPRISE/index.html.tmp since it should be rejected.\n","\n","--2024-11-16 13:57:25--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731745471.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12825 (13K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731745471.jpg’\n","\n","ANGER/ANGER_1731745 100%[===================>]  12.52K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:25 (292 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731745471.jpg’ saved [12825/12825]\n","\n","--2024-11-16 13:57:25--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731755893.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 26194 (26K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731755893.jpg’\n","\n","ANGER/ANGER_1731755 100%[===================>]  25.58K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:25 (305 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731755893.jpg’ saved [26194/26194]\n","\n","--2024-11-16 13:57:25--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731755969.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11171 (11K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731755969.jpg’\n","\n","ANGER/ANGER_1731755 100%[===================>]  10.91K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:25 (321 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731755969.jpg’ saved [11171/11171]\n","\n","--2024-11-16 13:57:25--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731756160.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14316 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731756160.jpg’\n","\n","ANGER/ANGER_1731756 100%[===================>]  13.98K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:26 (391 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731756160.jpg’ saved [14316/14316]\n","\n","--2024-11-16 13:57:26--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731756410.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13588 (13K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731756410.jpg’\n","\n","ANGER/ANGER_1731756 100%[===================>]  13.27K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:26 (356 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731756410.jpg’ saved [13588/13588]\n","\n","--2024-11-16 13:57:26--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731756424.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13898 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731756424.jpg’\n","\n","ANGER/ANGER_1731756 100%[===================>]  13.57K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:26 (284 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731756424.jpg’ saved [13898/13898]\n","\n","--2024-11-16 13:57:26--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731756435.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17192 (17K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731756435.jpg’\n","\n","ANGER/ANGER_1731756 100%[===================>]  16.79K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:26 (244 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731756435.jpg’ saved [17192/17192]\n","\n","--2024-11-16 13:57:26--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731756448.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15322 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731756448.jpg’\n","\n","ANGER/ANGER_1731756 100%[===================>]  14.96K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:26 (311 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731756448.jpg’ saved [15322/15322]\n","\n","--2024-11-16 13:57:26--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731756471.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16890 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731756471.jpg’\n","\n","ANGER/ANGER_1731756 100%[===================>]  16.49K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:26 (205 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731756471.jpg’ saved [16890/16890]\n","\n","--2024-11-16 13:57:26--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731756489.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14295 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731756489.jpg’\n","\n","ANGER/ANGER_1731756 100%[===================>]  13.96K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:26 (390 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731756489.jpg’ saved [14295/14295]\n","\n","--2024-11-16 13:57:26--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731757963.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13592 (13K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731757963.jpg’\n","\n","ANGER/ANGER_1731757 100%[===================>]  13.27K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (315 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731757963.jpg’ saved [13592/13592]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731761967.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12209 (12K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731761967.jpg’\n","\n","ANGER/ANGER_1731761 100%[===================>]  11.92K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (326 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731761967.jpg’ saved [12209/12209]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731762004.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15550 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731762004.jpg’\n","\n","ANGER/ANGER_1731762 100%[===================>]  15.19K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (338 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731762004.jpg’ saved [15550/15550]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/ANGER/ANGER_1731762018.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16630 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1731762018.jpg’\n","\n","ANGER/ANGER_1731762 100%[===================>]  16.24K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (223 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1731762018.jpg’ saved [16630/16630]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756488.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13090 (13K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756488.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  12.78K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (309 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756488.jpg’ saved [13090/13090]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756503.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11044 (11K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756503.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  10.79K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (253 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756503.jpg’ saved [11044/11044]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756520.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12381 (12K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756520.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  12.09K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (301 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756520.jpg’ saved [12381/12381]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756576.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17099 (17K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756576.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  16.70K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:27 (265 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756576.jpg’ saved [17099/17099]\n","\n","--2024-11-16 13:57:27--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756586.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13868 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756586.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  13.54K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:28 (355 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756586.jpg’ saved [13868/13868]\n","\n","--2024-11-16 13:57:28--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756597.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15267 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756597.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  14.91K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:28 (396 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756597.jpg’ saved [15267/15267]\n","\n","--2024-11-16 13:57:28--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756617.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16710 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756617.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  16.32K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:28 (258 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756617.jpg’ saved [16710/16710]\n","\n","--2024-11-16 13:57:28--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731756630.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15013 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756630.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  14.66K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:28 (444 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731756630.jpg’ saved [15013/15013]\n","\n","--2024-11-16 13:57:28--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731762035.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15800 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731762035.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  15.43K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:28 (411 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731762035.jpg’ saved [15800/15800]\n","\n","--2024-11-16 13:57:28--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731762045.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13992 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731762045.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  13.66K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:28 (344 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731762045.jpg’ saved [13992/13992]\n","\n","--2024-11-16 13:57:28--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/DISGUST/DISGUST_1731762068.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13365 (13K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1731762068.jpg’\n","\n","DISGUST/DISGUST_173 100%[===================>]  13.05K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:28 (344 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1731762068.jpg’ saved [13365/13365]\n","\n","--2024-11-16 13:57:28--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731756556.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13933 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731756556.jpg’\n","\n","FEAR/FEAR_173175655 100%[===================>]  13.61K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:29 (359 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731756556.jpg’ saved [13933/13933]\n","\n","--2024-11-16 13:57:29--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731756573.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17788 (17K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731756573.jpg’\n","\n","FEAR/FEAR_173175657 100%[===================>]  17.37K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:29 (258 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731756573.jpg’ saved [17788/17788]\n","\n","--2024-11-16 13:57:29--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731756617.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19442 (19K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731756617.jpg’\n","\n","FEAR/FEAR_173175661 100%[===================>]  18.99K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:29 (201 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731756617.jpg’ saved [19442/19442]\n","\n","--2024-11-16 13:57:29--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731756642.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14876 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731756642.jpg’\n","\n","FEAR/FEAR_173175664 100%[===================>]  14.53K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:29 (298 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731756642.jpg’ saved [14876/14876]\n","\n","--2024-11-16 13:57:29--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731756653.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16073 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731756653.jpg’\n","\n","FEAR/FEAR_173175665 100%[===================>]  15.70K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:29 (273 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731756653.jpg’ saved [16073/16073]\n","\n","--2024-11-16 13:57:29--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731756661.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10350 (10K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731756661.jpg’\n","\n","FEAR/FEAR_173175666 100%[===================>]  10.11K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:29 (256 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731756661.jpg’ saved [10350/10350]\n","\n","--2024-11-16 13:57:29--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731756670.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16544 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731756670.jpg’\n","\n","FEAR/FEAR_173175667 100%[===================>]  16.16K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:29 (270 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731756670.jpg’ saved [16544/16544]\n","\n","--2024-11-16 13:57:29--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731762088.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15345 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731762088.jpg’\n","\n","FEAR/FEAR_173176208 100%[===================>]  14.99K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (334 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731762088.jpg’ saved [15345/15345]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/FEAR/FEAR_1731762105.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14236 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1731762105.jpg’\n","\n","FEAR/FEAR_173176210 100%[===================>]  13.90K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (330 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1731762105.jpg’ saved [14236/14236]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731745416.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14924 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731745416.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  14.57K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (348 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731745416.jpg’ saved [14924/14924]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731749851.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15810 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731749851.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  15.44K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (341 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731749851.jpg’ saved [15810/15810]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731749862.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16742 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731749862.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  16.35K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (246 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731749862.jpg’ saved [16742/16742]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731755880.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 23925 (23K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731755880.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  23.36K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (245 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731755880.jpg’ saved [23925/23925]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756627.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21767 (21K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756627.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  21.26K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (290 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756627.jpg’ saved [21767/21767]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756640.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20793 (20K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756640.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  20.31K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:30 (295 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756640.jpg’ saved [20793/20793]\n","\n","--2024-11-16 13:57:30--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756652.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21003 (21K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756652.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  20.51K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:31 (218 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756652.jpg’ saved [21003/21003]\n","\n","--2024-11-16 13:57:31--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756737.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15054 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756737.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  14.70K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:31 (255 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756737.jpg’ saved [15054/15054]\n","\n","--2024-11-16 13:57:31--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756750.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14851 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756750.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  14.50K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:31 (293 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756750.jpg’ saved [14851/14851]\n","\n","--2024-11-16 13:57:31--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756761.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15198 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756761.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  14.84K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:31 (296 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756761.jpg’ saved [15198/15198]\n","\n","--2024-11-16 13:57:31--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756770.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12270 (12K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756770.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  11.98K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:31 (277 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756770.jpg’ saved [12270/12270]\n","\n","--2024-11-16 13:57:31--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756782.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13107 (13K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756782.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  12.80K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:31 (229 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756782.jpg’ saved [13107/13107]\n","\n","--2024-11-16 13:57:31--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756792.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12187 (12K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756792.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  11.90K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:31 (305 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731756792.jpg’ saved [12187/12187]\n","\n","--2024-11-16 13:57:31--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731757926.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14748 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731757926.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  14.40K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (301 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731757926.jpg’ saved [14748/14748]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731762118.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15018 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731762118.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  14.67K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (325 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731762118.jpg’ saved [15018/15018]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/HAPPINESS/HAPPINESS_1731762131.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15383 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731762131.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]  15.02K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (365 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1731762131.jpg’ saved [15383/15383]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731755905.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 30030 (29K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731755905.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  29.33K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (335 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731755905.jpg’ saved [30030/30030]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756663.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21599 (21K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756663.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  21.09K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (290 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756663.jpg’ saved [21599/21599]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756676.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19637 (19K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756676.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  19.18K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (275 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756676.jpg’ saved [19637/19637]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756817.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14430 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756817.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  14.09K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (398 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756817.jpg’ saved [14430/14430]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756826.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17097 (17K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756826.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  16.70K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:32 (232 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756826.jpg’ saved [17097/17097]\n","\n","--2024-11-16 13:57:32--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756834.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 18909 (18K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756834.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  18.47K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:33 (273 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756834.jpg’ saved [18909/18909]\n","\n","--2024-11-16 13:57:33--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756848.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17545 (17K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756848.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  17.13K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:33 (290 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756848.jpg’ saved [17545/17545]\n","\n","--2024-11-16 13:57:33--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756859.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17697 (17K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756859.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  17.28K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:33 (249 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731756859.jpg’ saved [17697/17697]\n","\n","--2024-11-16 13:57:33--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731757903.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14399 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731757903.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  14.06K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:33 (315 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731757903.jpg’ saved [14399/14399]\n","\n","--2024-11-16 13:57:33--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731762142.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12373 (12K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731762142.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  12.08K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:33 (261 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731762142.jpg’ saved [12373/12373]\n","\n","--2024-11-16 13:57:33--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/NEUTRAL/NEUTRAL_1731762156.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12907 (13K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731762156.jpg’\n","\n","NEUTRAL/NEUTRAL_173 100%[===================>]  12.60K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:33 (284 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1731762156.jpg’ saved [12907/12907]\n","\n","--2024-11-16 13:57:33--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756695.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21600 (21K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756695.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  21.09K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:33 (292 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756695.jpg’ saved [21600/21600]\n","\n","--2024-11-16 13:57:33--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756872.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16051 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756872.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  15.67K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:34 (345 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756872.jpg’ saved [16051/16051]\n","\n","--2024-11-16 13:57:34--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756897.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15929 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756897.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  15.56K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:34 (261 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756897.jpg’ saved [15929/15929]\n","\n","--2024-11-16 13:57:34--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756907.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14237 (14K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756907.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  13.90K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:34 (214 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756907.jpg’ saved [14237/14237]\n","\n","--2024-11-16 13:57:34--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756929.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20366 (20K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756929.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  19.89K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:34 (75.3 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756929.jpg’ saved [20366/20366]\n","\n","--2024-11-16 13:57:34--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756941.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21305 (21K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756941.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  20.81K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:34 (322 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756941.jpg’ saved [21305/21305]\n","\n","--2024-11-16 13:57:34--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756953.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21253 (21K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756953.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  20.75K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:34 (227 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756953.jpg’ saved [21253/21253]\n","\n","--2024-11-16 13:57:34--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731756979.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 12314 (12K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756979.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  12.03K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:34 (254 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731756979.jpg’ saved [12314/12314]\n","\n","--2024-11-16 13:57:34--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731757007.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19067 (19K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731757007.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  18.62K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (258 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731757007.jpg’ saved [19067/19067]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731757021.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 23406 (23K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731757021.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  22.86K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (359 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731757021.jpg’ saved [23406/23406]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731762171.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15300 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731762171.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  14.94K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (371 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731762171.jpg’ saved [15300/15300]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SADNESS/SADNESS_1731762184.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16373 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1731762184.jpg’\n","\n","SADNESS/SADNESS_173 100%[===================>]  15.99K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (195 MB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1731762184.jpg’ saved [16373/16373]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757000.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10677 (10K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757000.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  10.43K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (228 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757000.jpg’ saved [10677/10677]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757021.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10244 (10K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757021.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  10.00K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (236 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757021.jpg’ saved [10244/10244]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757031.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9714 (9.5K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757031.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]   9.49K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (256 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757031.jpg’ saved [9714/9714]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757044.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9736 (9.5K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757044.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]   9.51K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:35 (235 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757044.jpg’ saved [9736/9736]\n","\n","--2024-11-16 13:57:35--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757053.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7321 (7.1K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757053.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]   7.15K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:36 (2.87 GB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757053.jpg’ saved [7321/7321]\n","\n","--2024-11-16 13:57:36--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757057.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19171 (19K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757057.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  18.72K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:36 (217 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757057.jpg’ saved [19171/19171]\n","\n","--2024-11-16 13:57:36--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757071.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16886 (16K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757071.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  16.49K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:36 (258 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757071.jpg’ saved [16886/16886]\n","\n","--2024-11-16 13:57:36--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757086.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19087 (19K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757086.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  18.64K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:36 (265 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757086.jpg’ saved [19087/19087]\n","\n","--2024-11-16 13:57:36--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757095.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10008 (9.8K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757095.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]   9.77K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:36 (229 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757095.jpg’ saved [10008/10008]\n","\n","--2024-11-16 13:57:36--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757118.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10913 (11K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757118.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  10.66K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:36 (259 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757118.jpg’ saved [10913/10913]\n","\n","--2024-11-16 13:57:36--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731757132.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9318 (9.1K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757132.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]   9.10K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:36 (210 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731757132.jpg’ saved [9318/9318]\n","\n","--2024-11-16 13:57:36--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731762207.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15026 (15K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731762207.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  14.67K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:37 (345 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731762207.jpg’ saved [15026/15026]\n","\n","--2024-11-16 13:57:37--  https://aiml-sandbox.talentsprint.com/expression_detection/b23h4g19/captured_images_with_Expression/SURPRISE/SURPRISE_1731762222.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11872 (12K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731762222.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]  11.59K  --.-KB/s    in 0s      \n","\n","2024-11-16 13:57:37 (296 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1731762222.jpg’ saved [11872/11872]\n","\n","FINISHED --2024-11-16 13:57:37--\n","Total wall clock time: 13s\n","Downloaded: 94 files, 1.3M in 0.005s (272 MB/s)\n"]}]},{"cell_type":"code","metadata":{"id":"B4-clpEl-1RF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731765468144,"user_tz":-330,"elapsed":478,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"1849b645-9b37-4d5f-c874-f95905da09e5"},"source":["%ls"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mcaptured_images_with_Expression\u001b[0m/  Expression_data.zip   \u001b[01;34m__MACOSX\u001b[0m/\n","\u001b[01;34mExpression_data\u001b[0m/                  expression_model.pth  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","source":["# prompt: provide code for loading the team expression data. Note: Use the same transform which used for Expression_Data\n","\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# Use the same transform as used for Expression_Data\n","transform = transforms.Compose([\n","    #transforms.Grayscale(num_output_channels=1),\n","    transforms.Resize((48, 48)),\n","    transforms.ToTensor(),\n","    #transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Load the team expression data.  Replace 'path/to/team/data'\n","# with the actual path to your team's image directory.\n","team_dataset = dset.ImageFolder(root='/content/captured_images_with_Expression', transform=transform)\n","team_loader = DataLoader(team_dataset, batch_size=32, shuffle=False) # No need to shuffle for testing\n","\n","# Now you have team_loader, which you can use to iterate over your team's data.\n","# Example usage:\n","# for images, labels in team_loader:\n","#     # Process the images and labels here\n","#     ..."],"metadata":{"id":"2I622NXoRFqx","executionInfo":{"status":"ok","timestamp":1731765527694,"user_tz":-330,"elapsed":651,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#\n","# YOUR CODE HERE for loading the team expression data. Note: Use the same transform which used for Expression_Data.\n","# YOU CODE HERE for Dataloader: provide code to  team_loader, which you can use to iterate over your team's data.\n","\n","# Assuming the necessary imports and model definition are already present in the provided code.\n","\n","# The team_loader is already defined in the provided code:\n","\n","# team_dataset = dset.ImageFolder(root='/content/captured_images_with_Expression', transform=transform)\n","# team_loader = DataLoader(team_dataset, batch_size=32, shuffle=False) # No need to shuffle for testing\n","\n","# Example usage of team_loader (already present in the provided code):\n","for images, labels in team_loader:\n","    # Process the images and labels here\n","    images = images.to(device)  # Move images to the device (GPU or CPU)\n","    outputs = model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","\n","    # Print or store predictions (example)\n","    print(\"Predicted labels:\", predicted)\n","    print(\"Actual Labels:\", labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzdcRj5qRQ8X","executionInfo":{"status":"ok","timestamp":1731765591707,"user_tz":-330,"elapsed":549,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"671d674c-0934-4e22-8336-9696f2d30fb3"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted labels: tensor([0, 4, 4, 4, 0, 0, 4, 2, 5, 0, 0, 0, 0, 0, 4, 4, 0, 1, 5, 3, 5, 3, 0, 0,\n","        4, 2, 1, 5, 4, 1, 4, 4], device='cuda:0')\n","Actual Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 2, 2, 2, 2, 2, 2, 2])\n","Predicted labels: tensor([0, 4, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 5, 4, 4, 1,\n","        4, 4, 0, 4, 4, 4, 4, 4], device='cuda:0')\n","Actual Labels: tensor([2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 5, 5, 5])\n","Predicted labels: tensor([4, 3, 4, 4, 1, 5, 3, 4, 4, 2, 3, 3, 4, 4, 4, 5, 6, 1, 3, 3, 6, 2],\n","       device='cuda:0')\n","Actual Labels: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n"]}]},{"cell_type":"code","metadata":{"id":"rfZ0omsOJsp-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731765656952,"user_tz":-330,"elapsed":430,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"6cbd44f5-948b-4740-b1fb-e7a56ea5acc6"},"source":["# prompt: provide code for getting the CNN representation of your team data with expression. Optimize the CNN model for predicting the labels of expressions correctly\n","# # Note: If the CNN Model is not performing as expected, then you can add your Team Data to the Existing Training Data and Re-Train the Model.\n","\n","import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as dset\n","\n","# Assuming the necessary imports and model definition are already present in the provided code.\n","# The model and device are already defined in the preceding code.\n","\n","# Use the same transform as used for Expression_Data\n","transform = transforms.Compose([\n","    transforms.Resize((48, 48)),\n","    transforms.ToTensor(),\n","])\n","\n","# Load the team expression data.  Replace 'path/to/team/data'\n","# with the actual path to your team's image directory.\n","# The path is already corrected in the given code. No change needed here.\n","team_dataset = dset.ImageFolder(root='/content/captured_images_with_Expression', transform=transform)\n","team_loader = DataLoader(team_dataset, batch_size=32, shuffle=False) # No need to shuffle for testing\n","\n","\n","# Get CNN representations and optimize the model:\n","# Example usage of team_loader to get CNN representations and optimize the model:\n","\n","# Iterate through the team data\n","for images, labels in team_loader:\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # Get CNN representations\n","    with torch.no_grad(): # We don't need to calculate gradients during inference\n","        cnn_representations = model.forward(images)\n","        # You can now use these representations for further analysis or tasks.\n","        #print(cnn_representations)\n","\n","\n","    # Optimize the model based on the team data (if needed)\n","    # Example: if you have additional team data and labels, fine-tune:\n","\n","    outputs = model(images) # Forward pass\n","    loss = criterion(outputs, labels)  # Calculate Loss\n","\n","    optimizer.zero_grad() # Reset optimizer\n","    loss.backward()         # Backward pass\n","    optimizer.step()        # Update model parameters\n","\n","\n","    _, predicted = torch.max(outputs.data, 1)\n","    print(\"Predicted labels:\", predicted)\n","    print(\"Actual Labels:\", labels)\n","\n","\n","# (Optional) Save the fine-tuned model\n","torch.save(model.state_dict(), 'fine_tuned_expression_model.pth')"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted labels: tensor([0, 4, 4, 4, 0, 0, 4, 2, 5, 0, 0, 0, 0, 0, 4, 4, 0, 1, 5, 3, 5, 3, 0, 0,\n","        4, 2, 1, 5, 4, 1, 4, 4], device='cuda:0')\n","Actual Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n","Predicted labels: tensor([0, 4, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 4, 5, 4, 4, 1,\n","        4, 4, 0, 4, 4, 4, 4, 4], device='cuda:0')\n","Actual Labels: tensor([2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n","        4, 4, 4, 4, 4, 5, 5, 5], device='cuda:0')\n","Predicted labels: tensor([4, 3, 4, 4, 1, 5, 3, 4, 4, 2, 3, 3, 4, 4, 4, 5, 6, 1, 3, 3, 6, 2],\n","       device='cuda:0')\n","Actual Labels: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n","       device='cuda:0')\n"]}]},{"cell_type":"markdown","metadata":{"id":"OGixO_z6Gf-Y"},"source":["**Save your trained model**\n","\n","* Save the state dictionary of the classifier (use pytorch only), It will be useful in\n","integrating model to the mobile app\n","\n"," [Hint](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"]},{"cell_type":"code","metadata":{"id":"A7KAIpLsI4Uj"},"source":["### YOUR CODE HERE for saving the CNN model"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: provide code to save the CNN model\n","\n","# Save the model\n","torch.save(model.state_dict(), 'expression_model.pth')"],"metadata":{"id":"I4FEcXxlSmVI","executionInfo":{"status":"ok","timestamp":1731765909018,"user_tz":-330,"elapsed":421,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsCHKXubHAJB"},"source":["**Download your trained model**\n","* Given the path of model file the following code downloads it through the browser"]},{"cell_type":"code","metadata":{"id":"BDmWXfPaHJZG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1731765937041,"user_tz":-330,"elapsed":11,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"e550ffb7-05c8-49a3-a03c-25840ef0840b"},"source":["from google.colab import files\n","files.download('/content/expression_model.pth')"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_9833d92d-2c09-4ef4-baa7-1be759b7fa30\", \"expression_model.pth\", 22380324)"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"R7ccsM_ZISWj"},"source":["##**Stage 5 (Anti Face Spoofing): (5 marks)**\n","\n","\n","---\n","\n","\n","\n","The objective of anti face spoofing is to be able to unlock (say) a screen not just by your image\n","(which can be easily be spoofed with a photograph of yours) but by a switch in the expression\n","demanded by the Mobile App (which is much less probable to mimic)\n","* **Grading scheme**:\n","> * **Anti Face Spoofing**: (5M Only if both the cases mentioned below are achieved)\n",">>* **Unlock**: Correct face + Correct Demanded Expression\n",">>* **Stay Locked**: Correct face + Incorrect Demanded Expression (as you might imagine there are multiple other such possibilities, which you are free to explore)"]},{"cell_type":"code","metadata":{"id":"E9Wl6ZCEJ8gr"},"source":["# Test in your mobile app and see if it gets unlock."],"execution_count":null,"outputs":[]}]}