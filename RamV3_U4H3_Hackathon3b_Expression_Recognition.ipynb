{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17OCjE3aFnXD_SP_hUMsDxPDEibRJ6IPs","timestamp":1731760456169},{"file_id":"1cne24jP7FmCKSz1hzv3lSj1nQBN86YfA","timestamp":1731758508039},{"file_id":"1aouca44eWv1DNkYe7bNp85BE_zxXdyxu","timestamp":1731738769877}]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SZIubkln0AI2"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"a8pCC9bE2blm"},"source":["Automated facial expression recognition provides an objective assessment of emotions. Human based assessment of emotions has many limitations and biases and automated facial expression technology has been found to deliver a better level of insight into behavior patterns. Emotion detection from facial expressions using AI is useful in automatically measuring consumers’ engagement with their content and brands, audience engagement for advertisements, customer satisfaction in the retail sector, psychological analyses, law enforcement etc."]},{"cell_type":"code","metadata":{"id":"RURW82wHOKnb","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1731762445578,"user_tz":-330,"elapsed":630,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"fff1215a-c17a-45ba-c7a5-6d208bacdfca"},"source":["#@title Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Hackathon3b_expression_recognition.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Hackathon3b_expression_recognition.mp4\" type=\"video/mp4\">\n","</video>\n"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"CrBFZZit7ES0"},"source":["**Objectives:**\n","\n","**Stage 4 (15 Marks):** Train a CNN Model and perform Expression Recognition in the EFR Mobile App.\n","\n","**Stage 5 (5 Marks):** Test for Anti-Face Spoofing on the EFR Mobile App."]},{"cell_type":"markdown","metadata":{"id":"BGq6XpvhFynP"},"source":["##**Stage 4 (15 Marks)**\n","\n","**(i) Train a CNN Model for Expression Recognition on given Expression data  \n","(ii) Deploy the Model and Perform Expression Recognition on Team Data through the EFR Mobile App**\n","\n","\n","---\n","\n","\n","* Define and train a CNN for expression recognition for the data under folder \"Expression_data\" which segregated on expression basis.\n","* Collect your team data using EFR application and test your model on the same and optimize the CNN architecture for predicting the respective labels of the images.\n","* Save and Download the trained expression model and upload them in the ftp server (refer to [Filezilla Installation and Configuration document](https://drive.google.com/file/d/19UIKpyVK4r12Dxklo8quQdZQ31PWpiKM/view?usp=drive_link)).\n","\n","* Update the **“exp_recognition.py”** file in the server. Open the files in the terminal (Command prompt) and provide the code for predicting the expression on the face (Note: To define the architecture of your trained model, you'll need to define it in the file **\"exp_recognition_model.py\"**).\n","\n","* Test your model on the mobile app for Expression Recognition and Sequence Expression. Your team can also see your results in your terminal.\n","\n","\n","* Grading Scheme:\n","> * Expression Recognition (12M): If the functionality is returning expression class correctly for the face using the mobile app’s “Expression Recognition” functionality\n","> * Sequence Expression (3M): Get three consecutive correct Expressions using the mobile app’s “Sequence Expressions” functionality"]},{"cell_type":"markdown","metadata":{"id":"3e0e3sFh0JZJ"},"source":["**Download the dataset**"]},{"cell_type":"code","metadata":{"id":"Kv0xxq_d0Qb_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731763022857,"user_tz":-330,"elapsed":232912,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"8ae563d6-4a7c-4fdf-d75e-9656f2d10bd1"},"source":["#@title Run this cell to download the dataset\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","\n","notebook=\"M3_Hackathon\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx wget wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Expression_data.zip\")\n","\n","    ipython.magic(\"sx unzip Expression_data.zip\")\n","\n","    ipython.magic(\"sx pip install torch==2.5.1 -f https://download.pytorch.org/whl/cu100/stable\")\n","    ipython.magic(\"sx pip install torchvision==0.11\")\n","    ipython.magic(\"sx pip install opencv-python\")\n","    print (\"Setup completed successfully\")\n","    return\n","setup()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Setup completed successfully\n"]}]},{"cell_type":"markdown","metadata":{"id":"NhLFY4n6BwIj"},"source":["**Dataset attributes:**\n","\n","During the setup you have downloaded the Expression data:\n","\n","* **Expression_data**: In this folder, the images are segregrated in terms of Expression\n","> * Expressions available: ANGER, DISGUST, FEAR, HAPPINESS, NEUTRAL, SADNESS, SURPRISE\n","> * Each class is organised as one folder\n","> * There are ~18000 total images in the training data and ~4500 total images in the testing data"]},{"cell_type":"code","metadata":{"id":"9llRIwuyNxMZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731763027295,"user_tz":-330,"elapsed":581,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"6c09ed50-6324-4e44-bcb3-aba1f8fffe0a"},"source":["%ls"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mExpression_data\u001b[0m/  Expression_data.zip  \u001b[01;34m__MACOSX\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"markdown","metadata":{"id":"RC9IMSrJGTjK"},"source":["**Imports: All the imports are defined here**\n","\n"]},{"cell_type":"code","metadata":{"id":"Lj0Yjxe8G46e","executionInfo":{"status":"ok","timestamp":1731763034937,"user_tz":-330,"elapsed":5891,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"source":["%matplotlib inline\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader,Dataset\n","import matplotlib.pyplot as plt\n","import torchvision.utils\n","import numpy as np\n","import random\n","from PIL import Image\n","\n","from torch.autograd import Variable\n","import PIL.ImageOps\n","\n","from torch import optim\n","import torch.nn.functional as F\n","import os\n","import warnings\n","from time import sleep\n","import sys\n","warnings.filterwarnings('ignore')"],"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qnBncXo_-Fny","executionInfo":{"status":"ok","timestamp":1731762699725,"user_tz":-330,"elapsed":43,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWe3lAlQk5U0"},"source":["For the following step, to obtain hints on building a CNN model for face expression, you may refer to this [article](https://drive.google.com/open?id=1P2rpaWW3tOtGGnw4dvtdZ4hjoc8iDNst)"]},{"cell_type":"markdown","metadata":{"id":"c86_KxLjmLT7"},"source":["**Define and train a CNN model for expression recognition**"]},{"cell_type":"code","source":["\n","# Define the custom ResNet-50 model\n","class CustomResNet50(nn.Module):\n","    def __init__(self, num_classes=7):\n","        super(CustomResNet50, self).__init__()\n","        # Load pretrained ResNet-50\n","        self.model = models.resnet50(pretrained=True)\n","\n","        # Modify the first convolutional layer to handle 48x48 inputs\n","        self.model.conv1 = nn.Conv2d(\n","            in_channels=3,  # Input channels (e.g., RGB images)\n","            out_channels=64,  # Output channels\n","            kernel_size=3,  # Smaller kernel size for small images\n","            stride=1,  # Reduced stride for smaller input size\n","            padding=1,  # Ensure size consistency\n","            bias=False\n","        )\n","\n","        # Update the fully connected layer for 7 classes\n","        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Initialize the model\n","num_classes = 7\n","model = CustomResNet50(num_classes=num_classes)\n","\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Print model summary\n","from torchsummary import summary\n","summary(model, input_size=(3, 48, 48))\n","\n","# Test with dummy input\n","dummy_input = torch.randn(32, 3, 48, 48).to(device)  # Batch size of 32\n","output = model(dummy_input)\n","print(f\"Output shape: {output.shape}\")  # Expected: [32, 7]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"0oamuvUa9_Cd","executionInfo":{"status":"ok","timestamp":1731763043576,"user_tz":-330,"elapsed":4003,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"8d80ee95-5ccd-4e09-d596-5b66ec546d25"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 125MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 48, 48]           1,728\n","       BatchNorm2d-2           [-1, 64, 48, 48]             128\n","              ReLU-3           [-1, 64, 48, 48]               0\n","         MaxPool2d-4           [-1, 64, 24, 24]               0\n","            Conv2d-5           [-1, 64, 24, 24]           4,096\n","       BatchNorm2d-6           [-1, 64, 24, 24]             128\n","              ReLU-7           [-1, 64, 24, 24]               0\n","            Conv2d-8           [-1, 64, 24, 24]          36,864\n","       BatchNorm2d-9           [-1, 64, 24, 24]             128\n","             ReLU-10           [-1, 64, 24, 24]               0\n","           Conv2d-11          [-1, 256, 24, 24]          16,384\n","      BatchNorm2d-12          [-1, 256, 24, 24]             512\n","           Conv2d-13          [-1, 256, 24, 24]          16,384\n","      BatchNorm2d-14          [-1, 256, 24, 24]             512\n","             ReLU-15          [-1, 256, 24, 24]               0\n","       Bottleneck-16          [-1, 256, 24, 24]               0\n","           Conv2d-17           [-1, 64, 24, 24]          16,384\n","      BatchNorm2d-18           [-1, 64, 24, 24]             128\n","             ReLU-19           [-1, 64, 24, 24]               0\n","           Conv2d-20           [-1, 64, 24, 24]          36,864\n","      BatchNorm2d-21           [-1, 64, 24, 24]             128\n","             ReLU-22           [-1, 64, 24, 24]               0\n","           Conv2d-23          [-1, 256, 24, 24]          16,384\n","      BatchNorm2d-24          [-1, 256, 24, 24]             512\n","             ReLU-25          [-1, 256, 24, 24]               0\n","       Bottleneck-26          [-1, 256, 24, 24]               0\n","           Conv2d-27           [-1, 64, 24, 24]          16,384\n","      BatchNorm2d-28           [-1, 64, 24, 24]             128\n","             ReLU-29           [-1, 64, 24, 24]               0\n","           Conv2d-30           [-1, 64, 24, 24]          36,864\n","      BatchNorm2d-31           [-1, 64, 24, 24]             128\n","             ReLU-32           [-1, 64, 24, 24]               0\n","           Conv2d-33          [-1, 256, 24, 24]          16,384\n","      BatchNorm2d-34          [-1, 256, 24, 24]             512\n","             ReLU-35          [-1, 256, 24, 24]               0\n","       Bottleneck-36          [-1, 256, 24, 24]               0\n","           Conv2d-37          [-1, 128, 24, 24]          32,768\n","      BatchNorm2d-38          [-1, 128, 24, 24]             256\n","             ReLU-39          [-1, 128, 24, 24]               0\n","           Conv2d-40          [-1, 128, 12, 12]         147,456\n","      BatchNorm2d-41          [-1, 128, 12, 12]             256\n","             ReLU-42          [-1, 128, 12, 12]               0\n","           Conv2d-43          [-1, 512, 12, 12]          65,536\n","      BatchNorm2d-44          [-1, 512, 12, 12]           1,024\n","           Conv2d-45          [-1, 512, 12, 12]         131,072\n","      BatchNorm2d-46          [-1, 512, 12, 12]           1,024\n","             ReLU-47          [-1, 512, 12, 12]               0\n","       Bottleneck-48          [-1, 512, 12, 12]               0\n","           Conv2d-49          [-1, 128, 12, 12]          65,536\n","      BatchNorm2d-50          [-1, 128, 12, 12]             256\n","             ReLU-51          [-1, 128, 12, 12]               0\n","           Conv2d-52          [-1, 128, 12, 12]         147,456\n","      BatchNorm2d-53          [-1, 128, 12, 12]             256\n","             ReLU-54          [-1, 128, 12, 12]               0\n","           Conv2d-55          [-1, 512, 12, 12]          65,536\n","      BatchNorm2d-56          [-1, 512, 12, 12]           1,024\n","             ReLU-57          [-1, 512, 12, 12]               0\n","       Bottleneck-58          [-1, 512, 12, 12]               0\n","           Conv2d-59          [-1, 128, 12, 12]          65,536\n","      BatchNorm2d-60          [-1, 128, 12, 12]             256\n","             ReLU-61          [-1, 128, 12, 12]               0\n","           Conv2d-62          [-1, 128, 12, 12]         147,456\n","      BatchNorm2d-63          [-1, 128, 12, 12]             256\n","             ReLU-64          [-1, 128, 12, 12]               0\n","           Conv2d-65          [-1, 512, 12, 12]          65,536\n","      BatchNorm2d-66          [-1, 512, 12, 12]           1,024\n","             ReLU-67          [-1, 512, 12, 12]               0\n","       Bottleneck-68          [-1, 512, 12, 12]               0\n","           Conv2d-69          [-1, 128, 12, 12]          65,536\n","      BatchNorm2d-70          [-1, 128, 12, 12]             256\n","             ReLU-71          [-1, 128, 12, 12]               0\n","           Conv2d-72          [-1, 128, 12, 12]         147,456\n","      BatchNorm2d-73          [-1, 128, 12, 12]             256\n","             ReLU-74          [-1, 128, 12, 12]               0\n","           Conv2d-75          [-1, 512, 12, 12]          65,536\n","      BatchNorm2d-76          [-1, 512, 12, 12]           1,024\n","             ReLU-77          [-1, 512, 12, 12]               0\n","       Bottleneck-78          [-1, 512, 12, 12]               0\n","           Conv2d-79          [-1, 256, 12, 12]         131,072\n","      BatchNorm2d-80          [-1, 256, 12, 12]             512\n","             ReLU-81          [-1, 256, 12, 12]               0\n","           Conv2d-82            [-1, 256, 6, 6]         589,824\n","      BatchNorm2d-83            [-1, 256, 6, 6]             512\n","             ReLU-84            [-1, 256, 6, 6]               0\n","           Conv2d-85           [-1, 1024, 6, 6]         262,144\n","      BatchNorm2d-86           [-1, 1024, 6, 6]           2,048\n","           Conv2d-87           [-1, 1024, 6, 6]         524,288\n","      BatchNorm2d-88           [-1, 1024, 6, 6]           2,048\n","             ReLU-89           [-1, 1024, 6, 6]               0\n","       Bottleneck-90           [-1, 1024, 6, 6]               0\n","           Conv2d-91            [-1, 256, 6, 6]         262,144\n","      BatchNorm2d-92            [-1, 256, 6, 6]             512\n","             ReLU-93            [-1, 256, 6, 6]               0\n","           Conv2d-94            [-1, 256, 6, 6]         589,824\n","      BatchNorm2d-95            [-1, 256, 6, 6]             512\n","             ReLU-96            [-1, 256, 6, 6]               0\n","           Conv2d-97           [-1, 1024, 6, 6]         262,144\n","      BatchNorm2d-98           [-1, 1024, 6, 6]           2,048\n","             ReLU-99           [-1, 1024, 6, 6]               0\n","      Bottleneck-100           [-1, 1024, 6, 6]               0\n","          Conv2d-101            [-1, 256, 6, 6]         262,144\n","     BatchNorm2d-102            [-1, 256, 6, 6]             512\n","            ReLU-103            [-1, 256, 6, 6]               0\n","          Conv2d-104            [-1, 256, 6, 6]         589,824\n","     BatchNorm2d-105            [-1, 256, 6, 6]             512\n","            ReLU-106            [-1, 256, 6, 6]               0\n","          Conv2d-107           [-1, 1024, 6, 6]         262,144\n","     BatchNorm2d-108           [-1, 1024, 6, 6]           2,048\n","            ReLU-109           [-1, 1024, 6, 6]               0\n","      Bottleneck-110           [-1, 1024, 6, 6]               0\n","          Conv2d-111            [-1, 256, 6, 6]         262,144\n","     BatchNorm2d-112            [-1, 256, 6, 6]             512\n","            ReLU-113            [-1, 256, 6, 6]               0\n","          Conv2d-114            [-1, 256, 6, 6]         589,824\n","     BatchNorm2d-115            [-1, 256, 6, 6]             512\n","            ReLU-116            [-1, 256, 6, 6]               0\n","          Conv2d-117           [-1, 1024, 6, 6]         262,144\n","     BatchNorm2d-118           [-1, 1024, 6, 6]           2,048\n","            ReLU-119           [-1, 1024, 6, 6]               0\n","      Bottleneck-120           [-1, 1024, 6, 6]               0\n","          Conv2d-121            [-1, 256, 6, 6]         262,144\n","     BatchNorm2d-122            [-1, 256, 6, 6]             512\n","            ReLU-123            [-1, 256, 6, 6]               0\n","          Conv2d-124            [-1, 256, 6, 6]         589,824\n","     BatchNorm2d-125            [-1, 256, 6, 6]             512\n","            ReLU-126            [-1, 256, 6, 6]               0\n","          Conv2d-127           [-1, 1024, 6, 6]         262,144\n","     BatchNorm2d-128           [-1, 1024, 6, 6]           2,048\n","            ReLU-129           [-1, 1024, 6, 6]               0\n","      Bottleneck-130           [-1, 1024, 6, 6]               0\n","          Conv2d-131            [-1, 256, 6, 6]         262,144\n","     BatchNorm2d-132            [-1, 256, 6, 6]             512\n","            ReLU-133            [-1, 256, 6, 6]               0\n","          Conv2d-134            [-1, 256, 6, 6]         589,824\n","     BatchNorm2d-135            [-1, 256, 6, 6]             512\n","            ReLU-136            [-1, 256, 6, 6]               0\n","          Conv2d-137           [-1, 1024, 6, 6]         262,144\n","     BatchNorm2d-138           [-1, 1024, 6, 6]           2,048\n","            ReLU-139           [-1, 1024, 6, 6]               0\n","      Bottleneck-140           [-1, 1024, 6, 6]               0\n","          Conv2d-141            [-1, 512, 6, 6]         524,288\n","     BatchNorm2d-142            [-1, 512, 6, 6]           1,024\n","            ReLU-143            [-1, 512, 6, 6]               0\n","          Conv2d-144            [-1, 512, 3, 3]       2,359,296\n","     BatchNorm2d-145            [-1, 512, 3, 3]           1,024\n","            ReLU-146            [-1, 512, 3, 3]               0\n","          Conv2d-147           [-1, 2048, 3, 3]       1,048,576\n","     BatchNorm2d-148           [-1, 2048, 3, 3]           4,096\n","          Conv2d-149           [-1, 2048, 3, 3]       2,097,152\n","     BatchNorm2d-150           [-1, 2048, 3, 3]           4,096\n","            ReLU-151           [-1, 2048, 3, 3]               0\n","      Bottleneck-152           [-1, 2048, 3, 3]               0\n","          Conv2d-153            [-1, 512, 3, 3]       1,048,576\n","     BatchNorm2d-154            [-1, 512, 3, 3]           1,024\n","            ReLU-155            [-1, 512, 3, 3]               0\n","          Conv2d-156            [-1, 512, 3, 3]       2,359,296\n","     BatchNorm2d-157            [-1, 512, 3, 3]           1,024\n","            ReLU-158            [-1, 512, 3, 3]               0\n","          Conv2d-159           [-1, 2048, 3, 3]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 3, 3]           4,096\n","            ReLU-161           [-1, 2048, 3, 3]               0\n","      Bottleneck-162           [-1, 2048, 3, 3]               0\n","          Conv2d-163            [-1, 512, 3, 3]       1,048,576\n","     BatchNorm2d-164            [-1, 512, 3, 3]           1,024\n","            ReLU-165            [-1, 512, 3, 3]               0\n","          Conv2d-166            [-1, 512, 3, 3]       2,359,296\n","     BatchNorm2d-167            [-1, 512, 3, 3]           1,024\n","            ReLU-168            [-1, 512, 3, 3]               0\n","          Conv2d-169           [-1, 2048, 3, 3]       1,048,576\n","     BatchNorm2d-170           [-1, 2048, 3, 3]           4,096\n","            ReLU-171           [-1, 2048, 3, 3]               0\n","      Bottleneck-172           [-1, 2048, 3, 3]               0\n","AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n","          Linear-174                    [-1, 7]          14,343\n","          ResNet-175                    [-1, 7]               0\n","================================================================\n","Total params: 23,514,695\n","Trainable params: 23,514,695\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 52.64\n","Params size (MB): 89.70\n","Estimated Total Size (MB): 142.37\n","----------------------------------------------------------------\n","Output shape: torch.Size([32, 7])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TBIhcel_FXSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#   to train CNN model for Expression_Data\n","\n","\n","# Define the data transformations\n","transform = transforms.Compose([\n","    #transforms.Grayscale(num_output_channels=1),\n","    transforms.Resize((48, 48)),\n","    transforms.ToTensor(),\n","    #transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# Load the dataset\n","train_dataset = torchvision.datasets.ImageFolder(root='/content/Expression_data/Facial_expression_train', transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Training loop\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device) # Assuming your model is already defined as 'model'\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","total, correct = 0, 0\n","num_epochs = 35  # Adjust as needed\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        if (i + 1) % 100 == 0:\n","            accuracy = 100 * correct / total\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')\n","            total, correct = 0, 0  # Reset for next batch\n","    if accuracy >= 90:\n","        break\n","\n","print(\"Training finished.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1HYVIzjwnRr","executionInfo":{"status":"ok","timestamp":1731764291318,"user_tz":-330,"elapsed":1122319,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"bf4f37f6-31fc-4ac7-9c26-a32b3540cc57"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/35], Step [100/569], Loss: 1.9745, Accuracy: 27.19%\n","Epoch [1/35], Step [200/569], Loss: 1.6368, Accuracy: 30.38%\n","Epoch [1/35], Step [300/569], Loss: 1.7845, Accuracy: 33.50%\n","Epoch [1/35], Step [400/569], Loss: 1.6004, Accuracy: 32.53%\n","Epoch [1/35], Step [500/569], Loss: 1.5882, Accuracy: 31.94%\n","Epoch [2/35], Step [100/569], Loss: 1.6802, Accuracy: 31.83%\n","Epoch [2/35], Step [200/569], Loss: 1.8280, Accuracy: 31.44%\n","Epoch [2/35], Step [300/569], Loss: 1.5618, Accuracy: 33.53%\n","Epoch [2/35], Step [400/569], Loss: 1.6883, Accuracy: 32.78%\n","Epoch [2/35], Step [500/569], Loss: 1.7183, Accuracy: 35.06%\n","Epoch [3/35], Step [100/569], Loss: 1.5974, Accuracy: 34.75%\n","Epoch [3/35], Step [200/569], Loss: 1.8277, Accuracy: 33.97%\n","Epoch [3/35], Step [300/569], Loss: 1.7669, Accuracy: 33.97%\n","Epoch [3/35], Step [400/569], Loss: 1.5208, Accuracy: 33.00%\n","Epoch [3/35], Step [500/569], Loss: 1.7767, Accuracy: 36.44%\n","Epoch [4/35], Step [100/569], Loss: 1.4778, Accuracy: 35.42%\n","Epoch [4/35], Step [200/569], Loss: 1.7135, Accuracy: 34.50%\n","Epoch [4/35], Step [300/569], Loss: 1.5519, Accuracy: 36.03%\n","Epoch [4/35], Step [400/569], Loss: 1.7042, Accuracy: 37.03%\n","Epoch [4/35], Step [500/569], Loss: 1.7090, Accuracy: 36.66%\n","Epoch [5/35], Step [100/569], Loss: 1.3637, Accuracy: 38.08%\n","Epoch [5/35], Step [200/569], Loss: 1.5900, Accuracy: 38.12%\n","Epoch [5/35], Step [300/569], Loss: 1.4740, Accuracy: 37.06%\n","Epoch [5/35], Step [400/569], Loss: 1.6668, Accuracy: 37.75%\n","Epoch [5/35], Step [500/569], Loss: 1.5030, Accuracy: 37.56%\n","Epoch [6/35], Step [100/569], Loss: 1.6928, Accuracy: 32.47%\n","Epoch [6/35], Step [200/569], Loss: 1.6827, Accuracy: 30.38%\n","Epoch [6/35], Step [300/569], Loss: 1.8214, Accuracy: 30.47%\n","Epoch [6/35], Step [400/569], Loss: 1.5039, Accuracy: 34.06%\n","Epoch [6/35], Step [500/569], Loss: 1.6865, Accuracy: 35.09%\n","Epoch [7/35], Step [100/569], Loss: 1.6419, Accuracy: 36.78%\n","Epoch [7/35], Step [200/569], Loss: 1.5966, Accuracy: 36.62%\n","Epoch [7/35], Step [300/569], Loss: 1.6371, Accuracy: 38.34%\n","Epoch [7/35], Step [400/569], Loss: 1.6293, Accuracy: 40.03%\n","Epoch [7/35], Step [500/569], Loss: 1.5280, Accuracy: 36.56%\n","Epoch [8/35], Step [100/569], Loss: 1.6557, Accuracy: 39.31%\n","Epoch [8/35], Step [200/569], Loss: 1.6965, Accuracy: 39.91%\n","Epoch [8/35], Step [300/569], Loss: 1.5704, Accuracy: 38.22%\n","Epoch [8/35], Step [400/569], Loss: 1.5566, Accuracy: 40.28%\n","Epoch [8/35], Step [500/569], Loss: 1.5746, Accuracy: 39.75%\n","Epoch [9/35], Step [100/569], Loss: 1.5459, Accuracy: 41.00%\n","Epoch [9/35], Step [200/569], Loss: 1.4165, Accuracy: 41.16%\n","Epoch [9/35], Step [300/569], Loss: 1.8663, Accuracy: 39.56%\n","Epoch [9/35], Step [400/569], Loss: 1.6394, Accuracy: 42.00%\n","Epoch [9/35], Step [500/569], Loss: 1.7456, Accuracy: 40.00%\n","Epoch [10/35], Step [100/569], Loss: 1.5248, Accuracy: 40.42%\n","Epoch [10/35], Step [200/569], Loss: 1.5521, Accuracy: 42.56%\n","Epoch [10/35], Step [300/569], Loss: 1.8408, Accuracy: 44.81%\n","Epoch [10/35], Step [400/569], Loss: 1.2657, Accuracy: 42.50%\n","Epoch [10/35], Step [500/569], Loss: 1.5625, Accuracy: 41.50%\n","Epoch [11/35], Step [100/569], Loss: 1.5208, Accuracy: 42.77%\n","Epoch [11/35], Step [200/569], Loss: 1.4719, Accuracy: 44.09%\n","Epoch [11/35], Step [300/569], Loss: 1.5615, Accuracy: 45.16%\n","Epoch [11/35], Step [400/569], Loss: 1.3202, Accuracy: 43.41%\n","Epoch [11/35], Step [500/569], Loss: 1.6240, Accuracy: 43.44%\n","Epoch [12/35], Step [100/569], Loss: 1.3029, Accuracy: 45.41%\n","Epoch [12/35], Step [200/569], Loss: 1.4406, Accuracy: 47.38%\n","Epoch [12/35], Step [300/569], Loss: 1.4725, Accuracy: 45.97%\n","Epoch [12/35], Step [400/569], Loss: 1.4113, Accuracy: 47.25%\n","Epoch [12/35], Step [500/569], Loss: 1.5117, Accuracy: 45.75%\n","Epoch [13/35], Step [100/569], Loss: 1.1727, Accuracy: 47.55%\n","Epoch [13/35], Step [200/569], Loss: 1.5900, Accuracy: 49.31%\n","Epoch [13/35], Step [300/569], Loss: 1.4839, Accuracy: 49.06%\n","Epoch [13/35], Step [400/569], Loss: 1.2407, Accuracy: 48.91%\n","Epoch [13/35], Step [500/569], Loss: 1.1229, Accuracy: 48.84%\n","Epoch [14/35], Step [100/569], Loss: 1.3087, Accuracy: 46.86%\n","Epoch [14/35], Step [200/569], Loss: 1.3983, Accuracy: 49.16%\n","Epoch [14/35], Step [300/569], Loss: 1.4761, Accuracy: 50.03%\n","Epoch [14/35], Step [400/569], Loss: 1.3027, Accuracy: 51.09%\n","Epoch [14/35], Step [500/569], Loss: 1.2915, Accuracy: 50.78%\n","Epoch [15/35], Step [100/569], Loss: 1.2173, Accuracy: 54.37%\n","Epoch [15/35], Step [200/569], Loss: 1.2185, Accuracy: 58.16%\n","Epoch [15/35], Step [300/569], Loss: 1.0376, Accuracy: 55.50%\n","Epoch [15/35], Step [400/569], Loss: 1.3055, Accuracy: 55.72%\n","Epoch [15/35], Step [500/569], Loss: 1.2842, Accuracy: 54.69%\n","Epoch [16/35], Step [100/569], Loss: 1.0128, Accuracy: 60.95%\n","Epoch [16/35], Step [200/569], Loss: 1.1202, Accuracy: 64.62%\n","Epoch [16/35], Step [300/569], Loss: 1.1722, Accuracy: 62.47%\n","Epoch [16/35], Step [400/569], Loss: 0.8290, Accuracy: 61.34%\n","Epoch [16/35], Step [500/569], Loss: 1.1603, Accuracy: 60.12%\n","Epoch [17/35], Step [100/569], Loss: 1.0009, Accuracy: 67.76%\n","Epoch [17/35], Step [200/569], Loss: 0.7986, Accuracy: 74.19%\n","Epoch [17/35], Step [300/569], Loss: 1.0950, Accuracy: 69.94%\n","Epoch [17/35], Step [400/569], Loss: 0.7044, Accuracy: 68.94%\n","Epoch [17/35], Step [500/569], Loss: 0.7596, Accuracy: 68.28%\n","Epoch [18/35], Step [100/569], Loss: 0.6012, Accuracy: 75.08%\n","Epoch [18/35], Step [200/569], Loss: 0.4361, Accuracy: 80.47%\n","Epoch [18/35], Step [300/569], Loss: 0.3737, Accuracy: 79.12%\n","Epoch [18/35], Step [400/569], Loss: 0.4736, Accuracy: 77.88%\n","Epoch [18/35], Step [500/569], Loss: 0.7464, Accuracy: 75.66%\n","Epoch [19/35], Step [100/569], Loss: 0.1690, Accuracy: 82.99%\n","Epoch [19/35], Step [200/569], Loss: 0.3471, Accuracy: 88.81%\n","Epoch [19/35], Step [300/569], Loss: 0.6852, Accuracy: 86.22%\n","Epoch [19/35], Step [400/569], Loss: 0.3155, Accuracy: 84.38%\n","Epoch [19/35], Step [500/569], Loss: 0.4929, Accuracy: 83.50%\n","Epoch [20/35], Step [100/569], Loss: 0.3478, Accuracy: 87.69%\n","Epoch [20/35], Step [200/569], Loss: 0.3767, Accuracy: 92.12%\n","Epoch [20/35], Step [300/569], Loss: 0.1148, Accuracy: 91.53%\n","Epoch [20/35], Step [400/569], Loss: 0.2012, Accuracy: 89.56%\n","Epoch [20/35], Step [500/569], Loss: 0.5327, Accuracy: 89.38%\n","Epoch [21/35], Step [100/569], Loss: 0.1707, Accuracy: 88.27%\n","Epoch [21/35], Step [200/569], Loss: 0.2386, Accuracy: 92.94%\n","Epoch [21/35], Step [300/569], Loss: 0.1144, Accuracy: 93.38%\n","Epoch [21/35], Step [400/569], Loss: 0.1080, Accuracy: 92.78%\n","Epoch [21/35], Step [500/569], Loss: 0.3596, Accuracy: 91.94%\n","Training finished.\n"]}]},{"cell_type":"markdown","metadata":{"id":"gNZCxw0wmRlI"},"source":["**Test your model and optimize CNN architecture for predicting the labels correctly**"]},{"cell_type":"code","source":["# Save the model\n","torch.save(model.state_dict(), 'expression_model.pth')"],"metadata":{"id":"qLbrkcD-0xf1","executionInfo":{"status":"ok","timestamp":1731764295972,"user_tz":-330,"elapsed":437,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"SzuYlK_8mbYy"},"source":["# YOUR CODE HERE for test evaluation\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = torchvision.datasets.ImageFolder(root='/content/Expression_data/Facial_expression_test', transform=transform)\n","test_loader = DataLoader(test_dataset , batch_size=32, shuffle=True)"],"metadata":{"id":"qFKWrebhJPUo","executionInfo":{"status":"ok","timestamp":1731764299445,"user_tz":-330,"elapsed":465,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","# Load the saved model\n","model = CustomResNet50()  # Assuming ExpressionCNN is defined in your code\n","model.load_state_dict(torch.load('expression_model.pth'))\n","model.eval()  # Set the model to evaluation mode\n","\n","# Define the device (GPU if available, otherwise CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Testing loop\n","correct = 0\n","total = 0\n","with torch.no_grad():  # Disable gradient calculations during testing\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","test_accuracy = 100 * correct / total\n","print(f\"Test Accuracy: {test_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BI77oY9gJK-Q","executionInfo":{"status":"ok","timestamp":1731764430290,"user_tz":-330,"elapsed":11723,"user":{"displayName":"ram kancharla","userId":"09698398101534700684"}},"outputId":"7a33a546-3221-4156-a1d7-fc0d337eda86"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 37.62%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pvw3vZ7kR9sR"},"source":["**Team Data Collection (activate the server first)**\n","\n","  - (This can be done on the day of the Hackathon once the login username and password are given)"]},{"cell_type":"markdown","metadata":{"id":"1ki9ee8YUFQr"},"source":["Activate the Server Access\n","* Open the terminal (Command Prompt)\n","* Login to SSH by typing **ssh (username)@aiml-sandbox1.talentsprint.com**. Give the login username which is given to you.\n","\n","Eg: `ssh b16h3gxx@aiml-sandbox1.talentsprint.com`\n","\n","  (If it is your first time connecting to the server from this computer, accept the connection by typing \"yes\".)\n","* After logging into SSH, please activate your virtual environment using the\n","command **source venv/bin/activate** and then press enter\n","* You can start the server by giving the command **sh runserver.sh** and then press enter.\n","* In order to collect team data in mobile app, ensure the server is active\n"]},{"cell_type":"markdown","metadata":{"id":"bHpqJDuFHVmL"},"source":["**Collect your team data using the EFR Mobile App and fine-tune the CNN for expression data on your team**\n","\n","Team Data Collection\n","\n","* Follow the \"Mobile_APP_Documentation\" to collect the Expression photos of your team. These will be stored in the server to which login is provided to you.\n","\n","[Mobile_APP_Documentation](https://drive.google.com/file/d/1F9SU-BwKViK_eZV2-P3pymvGUILUoVFf/view?usp=drive_link)\n","\n","\n","**Download your team expression data from the EFR app into your colab notebook using the links provided below**\n","\n","NOTE: Replace the string \"username\" with your login username (such as b16h3gxx) in the below cell for expression images.\n","\n","This data will be useful for testing the above trained cnn networks."]},{"cell_type":"code","metadata":{"id":"I8U0F_CDIhmh"},"source":["!wget -nH --recursive --no-parent --reject 'index.*' https://aiml-sandbox.talentsprint.com/expression_detection/username/captured_images_with_Expression/ --cut-dirs=3  -P ./captured_images_with_Expression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4-clpEl-1RF"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfZ0omsOJsp-"},"source":["# YOUR CODE HERE for loading the team expression data. Note: Use the same transform which used for Expression_Data.\n","# YOU CODE HERE for Dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9VE2o32KO4y"},"source":["# YOUR CODE HERE for getting the CNN representation of your team data with expression. Optimize the CNN model for predicting the labels of expressions correctly\n","# Note: If the CNN Model is not performing as expected, then you can add your Team Data to the Existing Training Data and Re-Train the Model."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGixO_z6Gf-Y"},"source":["**Save your trained model**\n","\n","* Save the state dictionary of the classifier (use pytorch only), It will be useful in\n","integrating model to the mobile app\n","\n"," [Hint](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"]},{"cell_type":"code","metadata":{"id":"A7KAIpLsI4Uj"},"source":["### YOUR CODE HERE for saving the CNN model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsCHKXubHAJB"},"source":["**Download your trained model**\n","* Given the path of model file the following code downloads it through the browser"]},{"cell_type":"code","metadata":{"id":"BDmWXfPaHJZG"},"source":["from google.colab import files\n","files.download('<model_file_path>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R7ccsM_ZISWj"},"source":["##**Stage 5 (Anti Face Spoofing): (5 marks)**\n","\n","\n","---\n","\n","\n","\n","The objective of anti face spoofing is to be able to unlock (say) a screen not just by your image\n","(which can be easily be spoofed with a photograph of yours) but by a switch in the expression\n","demanded by the Mobile App (which is much less probable to mimic)\n","* **Grading scheme**:\n","> * **Anti Face Spoofing**: (5M Only if both the cases mentioned below are achieved)\n",">>* **Unlock**: Correct face + Correct Demanded Expression\n",">>* **Stay Locked**: Correct face + Incorrect Demanded Expression (as you might imagine there are multiple other such possibilities, which you are free to explore)"]},{"cell_type":"code","metadata":{"id":"E9Wl6ZCEJ8gr"},"source":["# Test in your mobile app and see if it gets unlock."],"execution_count":null,"outputs":[]}]}